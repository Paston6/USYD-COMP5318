{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf553c0",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1 - Clothes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e3e5cc",
   "metadata": {},
   "source": [
    "**Name**: Xing Xing <br />\n",
    "**Title**: Comp5318 Assignment1 <br />\n",
    "**ID**: 500390560 <br />\n",
    "**Start Date**: 31 Aug 2021 <br />\n",
    "**Finish Date**: 10 Sep 2021 <br />\n",
    "\n",
    "**Applied algorithms**: <br />\n",
    "    1. KNN, <br />\n",
    "    2. SVM, <br />\n",
    "    3. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0cddc",
   "metadata": {},
   "source": [
    "# 1. Import Packages and system information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bb2bc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== System Information =====\n",
      "- System: Mac Darwin\n",
      "- Machine: x86_64\n",
      "- Processor: i386\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import platform\n",
    "import h5py\n",
    "import time\n",
    "import math\n",
    "\n",
    "# ignore warnings for clean output \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#===================================\n",
    "# Hardware and software information \n",
    "#===================================\n",
    "print(\"===== System Information =====\")\n",
    "\n",
    "import platform\n",
    "import psutil\n",
    "\n",
    "info = platform.uname()\n",
    "\n",
    "print(\"- System: Mac \" + info.system)\n",
    "print(\"- Machine: \"+info.machine)\n",
    "print(\"- Processor: \" + info.processor)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53c0cd3",
   "metadata": {},
   "source": [
    "# 2. Load Data\n",
    "load_train_data(): Load 30000 training data    <br />\n",
    "load_train_label(): Load 30000 training labels <br />\n",
    "load_test_data(): Load all 5000 test data    <br />\n",
    "load_test_labels(): Load all 5000 test labels    <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "253cf371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      " load_data(): Loading data\n",
      " load_data(): Loading complete\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# load training data from ./Input/train\n",
    "\n",
    "def load_train_data():\n",
    "    with h5py.File('./Input/train/images_training.h5','r') as H:\n",
    "        train_data = np.copy(H['datatrain'])\n",
    "    return train_data\n",
    "\n",
    "def load_train_label():\n",
    "    with h5py.File('./Input/train/labels_training.h5','r') as H:\n",
    "        train_label= np.copy(H['labeltrain'])\n",
    "    return train_label\n",
    "\n",
    "# load testing data from ./Input/test\n",
    "\n",
    "def load_test_data():\n",
    "    with h5py.File('./Input/test/images_testing.h5','r') as H:\n",
    "        test_data = np.copy(H['datatest'])\n",
    "    return test_data\n",
    "        \n",
    "def load_test_label():\n",
    "    with h5py.File('./Input/test/labels_testing_2000.h5','r') as H:\n",
    "        test_label= np.copy(H['labeltest'])\n",
    "    return test_label\n",
    "\n",
    "\n",
    "#===========\n",
    "# load data\n",
    "#===========\n",
    "print(\"========================================\")\n",
    "print(\" load_data(): Loading data\")\n",
    "\n",
    "# load testing data\n",
    "test_data = load_test_data()\n",
    "test_label = load_test_label()\n",
    "\n",
    "# load training data\n",
    "train_data = load_train_data()\n",
    "train_label = load_train_label()\n",
    "\n",
    "print(\" load_data(): Loading complete\")\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d55c7de",
   "metadata": {},
   "source": [
    "# 3. Preprocessing and preparation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46dd4d",
   "metadata": {},
   "source": [
    "Preprocessing Functions:<br />\n",
    "    1. show_one_image(row, n): showing 1d np array as an n*n image with pyplot <br />\n",
    "    2. STD(data): standardise entire data <br />\n",
    "    3. NORM(data): normalisation of entire data<br /> \n",
    "    4. PCA(data, n): dimension deduction to <br />\n",
    "    5. preprocessing: data -> STD data -> NORM data -> PCA data -> ready to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b45f145",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "preprocessing(): start preprocessing train_data ...\n",
      "   - STD (): start standardise ...\n",
      "   - NORM (): start normalise ...\n",
      "   - PCA(): start generting pca n = 0.90\n",
      "   - PCA(): start PCA transform on train\n",
      "preprocessing(): start preprocessing test_data ...\n",
      "   - STD (): start standardise ...\n",
      "   - NORM (): start normalise ...\n",
      "   - PCA(): start PCA transform\n",
      "preprocessing(): clean up ...\n",
      "   - clean_train_data shape:(30000, 131)\n",
      "   - clean_test_data shape:(5000, 131)\n",
      "preprocessing(): preprocessing complete ...\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "# Show an image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#=================\n",
    "# Show Image\n",
    "#=================\n",
    "\n",
    "# Showing one image\n",
    "def show_one_img(row, n = 28):\n",
    "    # reshape to n*n 2d np array\n",
    "    img = np.reshape(row, (n, n))\n",
    "    plt.imshow(img, cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    \n",
    "#=================\n",
    "# Preprocessing\n",
    "#=================\n",
    "\n",
    "# Standardise entire data\n",
    "def STD(data):\n",
    "    print(\"   - STD (): start standardise ...\")\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    return scaler.transform(data)\n",
    "\n",
    "# Normalise entire data \n",
    "def NORM(data):\n",
    "    print(\"   - NORM (): start normalise ...\")\n",
    "    transformer = Normalizer().fit(data)\n",
    "    return transformer.transform(data)\n",
    "\n",
    "#=====================\n",
    "# Dimension deduction\n",
    "#=====================\n",
    "\n",
    "# PCA dimension deduction with n*n size\n",
    "def PCA_deduction(data, n):\n",
    "    if (n > 1):\n",
    "        pca = PCA(n_components = n).fit(data)\n",
    "        data = pca.fit_transform(data)\n",
    "    return pca\n",
    "\n",
    "# find best pca parameter\n",
    "def find_best_pca():\n",
    "    # find out best n_componenets for PCA\n",
    "    n_comp = [0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.98]\n",
    "    explain_list = []\n",
    "    dim_list = []\n",
    "    for i in n_comp:\n",
    "        pca = PCA(n_components = i).fit(train_data)\n",
    "        # cumsum variance explained \n",
    "        explain_list.append(np.sum(pca.explained_variance_))\n",
    "        dim_list.append(pca.n_components_)\n",
    "\n",
    "    # show elbow image\n",
    "    plt.plot(dim_list, explain_list, color = \"Blue\", linewidth = 1.5)\n",
    "    plt.show()\n",
    "    \n",
    "    # As we could see from image below, the elbow position is between \n",
    "\n",
    "#==========================\n",
    "# Preprocess training data\n",
    "#==========================\n",
    "print(\"===================================================\")\n",
    "print(\"preprocessing(): start preprocessing train_data ...\")\n",
    "\n",
    "# Generate pca deductor = pca\n",
    "# (deduct train and test seperately prevent data leak)\n",
    "\n",
    "# standardisation on train_data\n",
    "std_train_data = STD(train_data)\n",
    "\n",
    "# normalisation on train_data\n",
    "norm_train_data = NORM(std_train_data)\n",
    "\n",
    "print(\"   - PCA(): start generting pca n = 0.90\")\n",
    "pca = PCA(n_components = 0.90).fit(norm_train_data)\n",
    "\n",
    "print(\"   - PCA(): start PCA transform on train\")\n",
    "clean_train_data = pca.transform(norm_train_data)\n",
    "\n",
    "#==========================\n",
    "# Preprocess testing data\n",
    "#==========================\n",
    "print(\"preprocessing(): start preprocessing test_data ...\")\n",
    "\n",
    "# standardisation on test_data\n",
    "std_test_data = STD(test_data)\n",
    "\n",
    "# normalisation on test_data\n",
    "norm_test_data = NORM(std_test_data)\n",
    "\n",
    "# pca deduction on test_data\n",
    "print(\"   - PCA(): start PCA transform\")\n",
    "clean_test_data = pca.transform(norm_test_data)\n",
    "\n",
    "\n",
    "# check data shape\n",
    "print(\"preprocessing(): clean up ...\")\n",
    "print(\"   - clean_train_data shape:\" + str(clean_train_data.shape))\n",
    "print(\"   - clean_test_data shape:\" + str(clean_test_data.shape))\n",
    "\n",
    "# finish preprocessing \n",
    "print(\"preprocessing(): preprocessing complete ...\")\n",
    "print(\"===================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d388713",
   "metadata": {},
   "source": [
    "# 4. Training Models \n",
    "1. KNN, <br />\n",
    "2. SVM, <br />\n",
    "3. Logistic Regression, <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5933001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions and libraries \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6478e7",
   "metadata": {},
   "source": [
    "# 4.1 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7df3288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#================\n",
    "# KNN Classifier \n",
    "#================\n",
    "def KNN(train_data, train_label, test_data, test_label, para):\n",
    "    # record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # build classifier and training\n",
    "    knn = KNeighborsClassifier(n_neighbors = para['n_neighbors'], metric = para['metric'], weights = para['weights'])\n",
    "    knn.fit(train_data, train_label)\n",
    "    \n",
    "     # testing\n",
    "    result = knn.predict(test_data)\n",
    "    \n",
    "    # record end time\n",
    "    end_time = time.time()\n",
    "    # record runing time\n",
    "    print(\"KNN(): time cost = \" + str(round((end_time - start_time)/60, 2)) + \" Minutes...\")\n",
    "    \n",
    "    # individual model result\n",
    "    score = accuracy_score(result[0:2000], test_label)\n",
    "    print(\"KNN(): test result: \" + str(score*100) + \"%\\n\")\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e6cf453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "KNN_stun(): Start KNN stun...\n",
      "KNN_stun(): time cost = 6.28 Minutes...\n",
      "KNN_stun(): \n",
      "    Best score: 0.8644333333333334\n",
      "    Best parameter: {'metric': 'minkowski', 'n_neighbors': 6, 'weights': 'distance'}\n",
      "    Best index: 6\n"
     ]
    }
   ],
   "source": [
    "#============================\n",
    "# KNN Stuning hyperparameter\n",
    "#============================\n",
    "\n",
    "# find best KNN parameters use grid search with 10-fold stratified cross validation\n",
    "def KNN_stun(train_data, train_label):\n",
    "    print(\"=============================\")\n",
    "    print(\"KNN_stun(): Start KNN stun...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # select possible and reasonable parameters\n",
    "    K = list(range(3,9))\n",
    "    metric = [\"minkowski\",\"euclidean\"]\n",
    "    weights = [\"distance\", \"uniform\"]\n",
    "    \n",
    "    # aggregate parameters (K, metric, weight)\n",
    "    param = { 'n_neighbors': K, 'metric': metric, 'weights': weights }\n",
    "    \n",
    "    # generate empty classifier \n",
    "    knn = KNeighborsClassifier(n_jobs = 8)\n",
    "    \n",
    "    # cv as parameter is default stratified n fold cross validation \n",
    "    GS_CV = GridSearchCV(estimator=knn, param_grid=param, scoring='accuracy', cv = 10, refit=True)\n",
    "    \n",
    "    # train models with different parameters \n",
    "    GS_CV.fit(train_data, train_label)\n",
    "    \n",
    "    # record\n",
    "    end_time = time.time()\n",
    "    print(\"KNN_stun(): time cost = \" + str(round((end_time - start_time)/60, 2)) + \" Minutes...\")\n",
    "    print(\"KNN_stun(): \")\n",
    "    \n",
    "    # Print result of the best parameter\n",
    "    print(\"    Best score: \" + str(GS_CV.best_score_))\n",
    "    print(\"    Best parameter: \" + str(GS_CV.best_params_))\n",
    "    print(\"    Best index: \" + str(GS_CV.best_index_))\n",
    "    \n",
    "    return GS_CV.best_params_\n",
    "\n",
    "#========================\n",
    "# Find KNN hyper parameter \n",
    "#========================\n",
    "KNN_best_para = KNN_stun(clean_train_data, train_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00229a05",
   "metadata": {},
   "source": [
    "# 4.2 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67438fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVM predict on test data\n",
    "def svm(train_data, train_label, test_data, test_label, para):\n",
    "    # record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # build classifier and training\n",
    "    svm = SVC(C = para['C'], kernel = para['kernel'], gamma = para['gamma'], max_iter = 2000)\n",
    "    svm.fit(train_data,train_label)\n",
    "    \n",
    "    # testing\n",
    "    result = svm.predict(test_data[0:2000])\n",
    "    \n",
    "    # record end time\n",
    "    end_time = time.time()\n",
    "    # record runing time\n",
    "    print(\"svm(): time cost = \" + str(round((end_time - start_time)/60, 2)) + \" Minutes...\")\n",
    "    \n",
    "    # individual model result\n",
    "    score = accuracy_score(result[0:2000], test_label)\n",
    "    print(\"svm(): test result: \" + str(score) + \"% \\n\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8fbfbee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "svm_stun(): Start svm stun...\n",
      "svm_stun(): time cost = 38.68 Minutes...\n",
      "svm_stun(): result: ...\n",
      "    Best score: 0.9019999999999999\n",
      "    Best parameter: {'C': 2.75, 'gamma': 2.8, 'kernel': 'rbf'}\n",
      "    Best index: 2\n"
     ]
    }
   ],
   "source": [
    "#============================\n",
    "# SVM Stuning hyperparameter\n",
    "#============================\n",
    "\n",
    "# find best SVM parameters use grid search with 10-fold stratified cross validation\n",
    "def svm_stun(clean_train_data, train_label):\n",
    "    print(\"=============================\")\n",
    "    print(\"svm_stun(): Start svm stun...\")\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Only keep the parameter set for last stun \n",
    "    # select reasonable parameters and aggregate (C, gamma, kernel)\n",
    "    param_grid = {'C': [2.7, 2.75, 2.775], \n",
    "             'gamma': [2.6, 2.7, 2.8] ,\n",
    "             'kernel': ['rbf']}\n",
    "    \n",
    "    # cv as parameter is default stratified n fold cross validation \n",
    "    # maximum iteration = 1500 times\n",
    "    GS_CV = GridSearchCV(SVC(max_iter = 1500),param_grid, cv = 10, refit=True)\n",
    "    GS_CV.fit(clean_train_data, train_label)\n",
    "    \n",
    "    # Record finish time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # calculate time cost\n",
    "    print(\"svm_stun(): time cost = \" + str(round((end_time - start_time)/60, 2)) + \" Minutes...\")\n",
    "    print(\"svm_stun(): result: ...\")\n",
    "    \n",
    "    # print result\n",
    "    print(\"    Best score: \" + str(GS_CV.best_score_))\n",
    "    print(\"    Best parameter: \" + str(GS_CV.best_params_))\n",
    "    print(\"    Best index: \" + str(GS_CV.best_index_))\n",
    "    \n",
    "    return GS_CV.best_params_\n",
    "\n",
    "# find hyper parameter of svm\n",
    "SVM_best_para = svm_stun(clean_train_data, train_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72fa38b",
   "metadata": {},
   "source": [
    "# 4.3 Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "926f8d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic_regression(clean_train_data, train_label, clean_test_data,test_label, para):\n",
    "    # record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # build classifier and training\n",
    "    \n",
    "    logi = LogisticRegression(multi_class=\"multinomial\",penalty = para[\"penalty\"], solver=para[\"solver\"], max_iter=1500)\n",
    "    logi.fit(train_data,train_label)\n",
    "    \n",
    "    # testing\n",
    "    result = logi.predict(test_data)\n",
    "    \n",
    "    # record end time\n",
    "    end_time = time.time()\n",
    "    # record runing time\n",
    "    print(\"logistic_regression(): time cost = \" + str(round((end_time - start_time)/60, 2)) + \" Minutes...\")\n",
    "    \n",
    "    # individual model result\n",
    "    score = accuracy_score(result[0:2000], test_label)\n",
    "    print(\"logistic_regression(): test result: \" + str(score) + \"%\\n\")\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9fd9be9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================\n",
      "logistic_regression_stun(): Start logistic_regression stun...\n",
      "logistic_regression_stun(): time cost = 2.02 Minutes...\n",
      "logistic_regression_stun(): \n",
      "    Best score: 0.8471666666666667\n",
      "    Best parameter: {'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "    Best index: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find hyper parameters for logistic regression \n",
    "def logistic_regression_stun(clean_train_data, train_label):\n",
    "    print(\"=============================================================\")\n",
    "    print(\"logistic_regression_stun(): Start logistic_regression stun...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # select possible and reasonable parameters\n",
    "    penalty = ['l2', 'elasticnet'] #default=’l2’\n",
    "    solver = [\"newton-cg\", \"lbfgs\", \"sag\", \"saga\"]\n",
    "    \n",
    "    # aggregate parameters (K, metric, weight)\n",
    "    param = { 'penalty': penalty, 'solver': solver }\n",
    "    \n",
    "    # generate empty classifier\n",
    "    logi = LogisticRegression(max_iter = 100000, n_jobs = 1)\n",
    "    \n",
    "    # cv as parameter is default stratified n fold cross validation \n",
    "    GS_CV = GridSearchCV(estimator=logi, param_grid=param, cv = 10, refit=True)\n",
    "    GS_CV.fit(clean_train_data, train_label)\n",
    "    \n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "    print(\"logistic_regression_stun(): time cost = \" + str(round((end_time - start_time)/60, 2)) + \" Minutes...\")\n",
    "    print(\"logistic_regression_stun(): \")\n",
    "    \n",
    "    # Print result of the best parameter\n",
    "    print(\"    Best score: \" + str(GS_CV.best_score_))\n",
    "    print(\"    Best parameter: \" + str(GS_CV.best_params_))\n",
    "    print(\"    Best index: \" + str(GS_CV.best_index_))\n",
    "    \n",
    "    return GS_CV.best_params_\n",
    "\n",
    "\n",
    "# get best parameters\n",
    "LR_best_para = logistic_regression_stun(clean_train_data, train_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee526ce3",
   "metadata": {},
   "source": [
    "# 5 Comparation of KNN, SVM, Logistic Reg performance\n",
    "**Default HyperParameters from previous running :**<br>\n",
    "Just in case if markers did not run the stun proccess in Part 4 which could use a lot of time, this part will use the hyperparameters which are obtained from previous running. Related information of these variables are shown below:<br>\n",
    "\n",
    "**KNN hyper parameter** <br>\n",
    "Variable name: default_KNN_best_para<br>\n",
    "{'metric': 'minkowski', 'n_neighbors': 6, 'weights': 'distance'} <br>\n",
    "<br>\n",
    "**SVM hyper parameter** <br>\n",
    "Variable name: default_SVM_best_para<br>\n",
    "{'C': 3, 'gamma': 3.0, 'kernel': 'rbf'} <br>\n",
    "<br>\n",
    "**Logistic Regression hyper parameter** <br>\n",
    "Variable name: default_LR_best_para<br>\n",
    "{'penalty': 'l2', 'solver': 'lbfgs'} <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "644cd78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start KNN with HyperParameters: {'metric': 'minkowski', 'n_neighbors': 6, 'weights': 'distance'}\n",
      "KNN(): time cost = 0.05 Minutes...\n",
      "KNN(): test result: 85.55%\n",
      "\n",
      "Start SVM with HyperParameters: {'C': 2, 'gamma': 2.8, 'kernel': 'rbf'}\n",
      "svm(): time cost = 0.47 Minutes...\n",
      "svm(): test result: 0.8845% \n",
      "\n",
      "Start Logistic regression with HyperParameters: {'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "logistic_regression(): time cost = 0.81 Minutes...\n",
      "logistic_regression(): test result: 0.8395%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#===============\n",
    "# compare model\n",
    "#===============\n",
    "\n",
    "# If marker did not run the stun proccess in Part 4 \n",
    "# It will use the hyperparameters which are obtained from previous running\n",
    "\n",
    "# Else, default best parameters are obtained from Part4 \n",
    "# which are KNN_stun(), SVM()_stun, logistic_regression_stun()\n",
    "\n",
    "\n",
    "\n",
    "# KNN best parameter from previous running\n",
    "default_KNN_best_para = {'metric': 'minkowski', 'n_neighbors': 6, 'weights': 'distance'}\n",
    "\n",
    "# SVM best parameter from previous running\n",
    "default_SVM_best_para = {'C': 2, 'gamma': 2.8, 'kernel': 'rbf'}\n",
    "\n",
    "# Logistic regression best parameter from previous running\n",
    "default_LR_best_para = {'penalty': 'l2', 'solver': 'lbfgs'} \n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Start KNN\n",
    "# ==========================\n",
    "\n",
    "# if stun result is kept\n",
    "if ('KNN_best_para' in locals().keys()):\n",
    "    print(\"Start KNN with HyperParameters: \" + str(KNN_best_para))\n",
    "    KNN_result = KNN(clean_train_data, train_label, clean_test_data, test_label, KNN_best_para)\n",
    "else:\n",
    "    print(\"Start KNN with HyperParameters: \" + str(default_KNN_best_para))\n",
    "    KNN_result = KNN(clean_train_data, train_label, clean_test_data, test_label, default_KNN_best_para)\n",
    "    \n",
    "\n",
    "# ==========================\n",
    "# Start SVM\n",
    "# ==========================\n",
    "\n",
    "# SVM algorithm and get 5000 predicted result\n",
    "if ('SVM_best_para' in locals().keys()):\n",
    "    print(\"Start SVM with HyperParameters: \" + str(SVM_best_para))\n",
    "    svm_result = svm(clean_train_data, train_label, clean_test_data, test_label, SVM_best_para)\n",
    "else:\n",
    "    print(\"Start SVM with HyperParameters: \" + str(default_SVM_best_para))\n",
    "    svm_result = svm(clean_train_data, train_label, clean_test_data, test_label, default_SVM_best_para)\n",
    "    \n",
    "\n",
    "# ==========================\n",
    "# Start Logistic regression\n",
    "# ==========================\n",
    "\n",
    "if ('LR_best_para' in locals().keys()):\n",
    "    print(\"Start Logistic regression with HyperParameters: \" + str(LR_best_para))\n",
    "    Logi_result = logistic_regression(clean_train_data, train_label, clean_test_data, test_label, LR_best_para)\n",
    "else:\n",
    "    print(\"Start Logistic regression with HyperParameters: \" + str(default_LR_best_para))\n",
    "    Logi_result = logistic_regression(clean_train_data, train_label, clean_test_data, test_label, default_LR_best_para)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d22a6",
   "metadata": {},
   "source": [
    "# 6. Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15d132a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "# Save output generated by best model \n",
    "\n",
    "\n",
    "\n",
    "# Path as: Output/predicted_labels.h5\n",
    "\n",
    "# assume output is the predicted labels from classifiers\n",
    "# (5000,)\n",
    "with h5py.File('Output/predicted_labels.h5','w') as H:\n",
    "    print(svm_result.shape)\n",
    "    H.create_dataset('Output',data=svm_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc4d4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
